# ğŸ¤– Support Knowledge Base Agent (Hybrid RAG System)

An intelligent **Support Knowledge Base Agent** built using a **hybrid Retrieval-Augmented Generation (RAG)** pipeline.  
The system allows users to upload documents (PDFs), retrieve relevant information using **keyword + semantic search**, and generate accurate answers using a **Groq LLM**, with clear source citations.

This project is designed to be **stable, modular, and interview-ready**, avoiding experimental tooling while following real-world engineering practices.

---

## ğŸš€ Setup & Installation

### Prerequisites
- Python **3.10+**
- Virtual environment (`venv`)
- Groq API Key

### Installation

```
# Clone the repository
git clone <your-repo-url>
cd Agentic-Support-Knowledge-Base-System

# Create virtual environment
python -m venv venv

# Activate environment
# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### ğŸ” Environment Variables
Create a .env file in the project root:

```
GROQ_API_KEY=your_groq_api_key_here
```

Only the Groq API key is required.
Embeddings are computed locally using HuggingFace models.

## ğŸƒ How to Run

### 1ï¸âƒ£ Start the Tool Server (FastAPI)
Run from the project root:

```
uvicorn mcp_server.server:app --host 0.0.0.0 --port 8000
```

The tool server exposes retrieval endpoints used by the frontend.

### 2ï¸âƒ£ Start the Streamlit App
Open a new terminal:

```
python -m streamlit run frontend/app.py
```

The app will open at:

```
http://localhost:8501
```

## ğŸ“‚ Document Ingestion Workflow

1. Upload PDFs using the Streamlit sidebar

2. Click Ingest Documents

3. Documents are:

â€¢ Loaded using PyPDFLoader

â€¢ Chunked with RecursiveCharacterTextSplitter

â€¢ Embedded using a free HuggingFace embedding model

â€¢ Stored locally in a FAISS vector index

â€¢ Embeddings are computed once and reused for all queries.

## ğŸ” RAG Pipeline (Hybrid Retrieval)

### Retrieval Strategy

The system uses hybrid retrieval:

â€¢ BM25 (keyword search) â€” exact term matching

â€¢ FAISS (semantic search) â€” vector similarity

Both signals are combined to improve retrieval quality and reduce hallucinations.

### Retrieval Flow

1. User query â†’ FastAPI tool server

2. Hybrid retrieval over indexed documents

3. Top relevant chunks returned to frontend

4. Context passed to the LLM  

## ğŸ§  Answer Generation

â€¢ LLM Provider: Groq

â€¢ Prompt is strictly grounded in retrieved context

â€¢ The model is instructed to answer only from provided documents

â€¢ The final answer includes the most relevant source citation

Example citation format:

```
Source:
- user_manual.pdf (page 3)
```

## ğŸ§± System Architecture

```
Streamlit Frontend
   |
   |  HTTP (requests)
   v
FastAPI Tool Server (MCP-style)
   |
   v
Hybrid RAG Backend
(BM25 + FAISS + HuggingFace Embeddings)
   |
   v
Groq LLM
```

## ğŸ“ Project Structure

```
Agentic Support Knowledge Base System/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ ingest.py        # Document loading & chunking
â”‚   â”œâ”€â”€ embeddings.py   # FAISS vector store handling
â”‚   â”œâ”€â”€ retriever.py    # Hybrid BM25 + FAISS retrieval
â”‚   â”œâ”€â”€ llm.py          # Groq LLM initialization
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ mcp_server/
â”‚   â””â”€â”€ server.py       # FastAPI-based tool server
â”‚
â”œâ”€â”€ mcp_client/
â”‚   â””â”€â”€ client.py       # HTTP client for tool calls
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py          # Streamlit UI
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # Uploaded PDFs
â”‚   â””â”€â”€ vectorstore/    # FAISS index
â”‚
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

All directories are valid Python packages (__init__.py included).

## âœ¨ Features

âœ… PDF-based knowledge ingestion
âœ… Hybrid retrieval (BM25 + semantic search)
âœ… Free local embeddings (no embedding API cost)
âœ… FastAPI-based tool server
âœ… Streamlit chat UI with history
âœ… Source citation for answers
âœ… Cached document loading for performance
âœ… Clean, modular, interview-ready codebase

## âš™ï¸ Design Choices & Rationale

â€¢ FastAPI instead of experimental MCP transports for stability

â€¢ Hybrid retrieval to improve recall and precision

â€¢ Local embeddings to avoid API costs

â€¢ No agent overengineering â€” focused on correctness and clarity

â€¢ Separation of concerns between UI, tools, and backend logic

## ğŸ“¦ Dependencies

Key libraries used:

â€¢ Streamlit

â€¢ FastAPI + Uvicorn

â€¢ LangChain

â€¢ FAISS

â€¢ Rank-BM25

â€¢ Sentence-Transformers

â€¢ Groq LLM

See requirements.txt for full list.


## ğŸ“„ License

This project is licensed under the MIT License.

ğŸ™ Acknowledgements

â€¢ LangChain for RAG abstractions

â€¢ HuggingFace for embedding models

â€¢ Groq for fast LLM inference

â€¢ Streamlit for rapid UI development

â€¢ FastAPI for reliable backend services
